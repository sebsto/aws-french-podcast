# Requirements Document

## Introduction

This document specifies the requirements for building an Amazon Bedrock Knowledge Base with S3 vector storage to enable semantic search across all podcast episode transcriptions. The system will automatically ingest new transcriptions as they are created and enrich them with episode metadata to provide comprehensive search results.

**Out of Scope:** This feature does NOT include transcription generation. Transcription files are already created by the existing podcast processing workflow (Amazon Transcribe via Step Functions). This feature focuses solely on indexing existing transcriptions into a searchable Knowledge Base.

## Glossary

- **Knowledge_Base**: Amazon Bedrock Knowledge Base service that provides semantic search capabilities over document collections
- **Vector_Store**: S3-based vector storage backend for the Knowledge Base (cost-optimized alternative to OpenSearch)
- **Transcription_File**: JSON file output from Amazon Transcribe stored at s3://aws-french-podcast-media/text/{episode}-transcribe.json
- **Episode_Metadata**: Structured information about a podcast episode including title, description, guests, publication date, and related links
- **Data_Source**: Bedrock Knowledge Base data source pointing to S3 location containing documents to index
- **Ingestion_Job**: Bedrock process that reads documents from the data source, generates embeddings, and stores vectors
- **Embedding_Model**: AI model that converts text into vector representations for semantic search (e.g., Amazon Titan Embeddings)
- **Metadata_Document**: Combined document containing both transcription text and episode metadata in a format suitable for Knowledge Base ingestion
- **EventBridge_Rule**: AWS EventBridge rule that triggers workflows when specific events occur

## Requirements

### Requirement 1: Knowledge Base Infrastructure

**User Story:** As a system administrator, I want to provision a Bedrock Knowledge Base with S3 vector storage, so that I can enable semantic search over podcast transcriptions while minimizing infrastructure costs.

#### Acceptance Criteria

1. THE System SHALL create a Bedrock Knowledge Base resource in the eu-central-1 region
2. THE System SHALL configure the Knowledge Base to use S3 as the vector store backend
3. THE System SHALL create an S3 bucket for vector storage with appropriate lifecycle policies
4. THE System SHALL configure the Knowledge Base with an appropriate embedding model (Amazon Titan Embeddings or similar)
5. THE System SHALL create necessary IAM roles and policies for Bedrock to access S3 resources
6. THE System SHALL deploy all infrastructure using AWS CDK TypeScript following existing project patterns

### Requirement 2: Data Source Configuration

**User Story:** As a system administrator, I want to configure a data source pointing to processed transcription documents, so that the Knowledge Base can ingest and index podcast content.

#### Acceptance Criteria

1. THE System SHALL create an S3 bucket or prefix for storing processed metadata documents
2. THE System SHALL configure a Bedrock Knowledge Base data source pointing to the processed documents location
3. THE System SHALL configure the data source with appropriate chunking strategy for podcast transcriptions
4. WHEN the data source is configured, THE System SHALL support incremental ingestion of new documents
5. THE System SHALL configure appropriate metadata fields for filtering and retrieval

### Requirement 3: Transcription Text Extraction

**User Story:** As a developer, I want to extract text from existing Amazon Transcribe JSON files, so that I can prepare transcription content for Knowledge Base ingestion.

**Note:** Transcription files are already generated by the existing workflow. This requirement only covers reading and extracting text from existing files.

#### Acceptance Criteria

1. WHEN an existing transcription file is provided, THE Transcription_Processor SHALL parse the JSON structure
2. WHEN parsing transcription JSON, THE Transcription_Processor SHALL extract the full transcript text from results.transcripts[0].transcript
3. IF the transcription file is malformed or missing required fields, THEN THE Transcription_Processor SHALL return a descriptive error
4. THE Transcription_Processor SHALL handle transcription files of varying lengths (up to several hours of audio)
5. WHEN extracting text, THE Transcription_Processor SHALL preserve the original text without modification

### Requirement 4: Metadata Enrichment

**User Story:** As a content manager, I want transcriptions enriched with episode metadata, so that search results include contextual information about each episode.

**Note:** Episode metadata is retrieved from the public RSS feed at https://francais.podcast.go-aws.com/web/feed.xml, not from local files.

#### Acceptance Criteria

1. WHEN processing an episode, THE Metadata_Enricher SHALL fetch episode metadata from the public RSS feed at https://francais.podcast.go-aws.com/web/feed.xml
2. THE Metadata_Enricher SHALL extract the following metadata fields from the RSS feed: episode number, title, description, publication date, author, guest names, guest titles, guest LinkedIn links, and related links
3. WHEN metadata is unavailable for an episode in the RSS feed, THE Metadata_Enricher SHALL use default values or skip optional fields
4. THE Metadata_Enricher SHALL combine transcription text with metadata into a single structured document
5. THE Metadata_Enricher SHALL format the combined document in a format suitable for Bedrock Knowledge Base ingestion (text, JSON, or markdown)
6. THE Metadata_Enricher SHALL cache RSS feed data to minimize HTTP requests during batch processing

### Requirement 5: Document Preparation

**User Story:** As a developer, I want to create properly formatted documents for Knowledge Base ingestion, so that Bedrock can effectively index and retrieve podcast content.

#### Acceptance Criteria

1. THE Document_Formatter SHALL create documents with both content and metadata sections
2. THE Document_Formatter SHALL include episode number as a filterable metadata field
3. THE Document_Formatter SHALL include publication date as a filterable metadata field
4. THE Document_Formatter SHALL include guest names as searchable content
5. THE Document_Formatter SHALL write formatted documents to the configured S3 data source location
6. WHEN writing documents, THE Document_Formatter SHALL use a consistent naming convention (e.g., {episode}.txt or {episode}.json)

### Requirement 6: Automatic Ingestion Trigger

**User Story:** As a system administrator, I want new transcriptions automatically added to the Knowledge Base when they become available, so that search results stay current without manual intervention.

**Note:** This integrates with the existing transcription workflow. When a transcription file is created by the existing Step Functions workflow, this feature will process it for the Knowledge Base.

#### Acceptance Criteria

1. WHEN a transcription file is created in s3://aws-french-podcast-media/text/ by the existing workflow, THE System SHALL trigger the document processing workflow
2. THE System SHALL integrate with the existing EventBridge rule for transcription completion events
3. WHEN the processing workflow completes, THE System SHALL trigger a Bedrock Knowledge Base ingestion job
4. THE System SHALL support both full ingestion (all episodes) and incremental ingestion (new episodes only)
5. IF an ingestion job fails, THEN THE System SHALL send an alert notification via SNS

### Requirement 7: Initial Data Ingestion

**User Story:** As a content manager, I want to ingest all existing transcriptions (341+ episodes) into the Knowledge Base, so that the complete podcast archive is searchable from day one.

**Note:** This is an ingestion process, not a migration. Source files remain in their original S3 locations.

#### Acceptance Criteria

1. THE Ingestion_Tool SHALL process all existing transcription files in s3://aws-french-podcast-media/text/
2. THE Ingestion_Tool SHALL fetch episode metadata from the public RSS feed at https://francais.podcast.go-aws.com/web/feed.xml
3. WHEN processing existing episodes, THE Ingestion_Tool SHALL handle missing or incomplete metadata gracefully
4. THE Ingestion_Tool SHALL generate formatted documents for all episodes
5. WHEN ingestion completes, THE Ingestion_Tool SHALL trigger a full Knowledge Base ingestion job
6. THE Ingestion_Tool SHALL provide progress reporting and error logging

### Requirement 8: Ingestion Job Management

**User Story:** As a developer, I want to programmatically trigger and monitor Knowledge Base ingestion jobs, so that I can ensure documents are successfully indexed.

#### Acceptance Criteria

1. THE Ingestion_Manager SHALL start a Bedrock Knowledge Base ingestion job when new documents are available
2. WHEN starting an ingestion job, THE Ingestion_Manager SHALL specify the data source to ingest
3. THE Ingestion_Manager SHALL monitor ingestion job status until completion
4. IF an ingestion job fails, THEN THE Ingestion_Manager SHALL log the failure reason and send an alert
5. WHEN an ingestion job completes successfully, THE Ingestion_Manager SHALL log the number of documents processed

### Requirement 9: Error Handling and Monitoring

**User Story:** As a system administrator, I want comprehensive error handling and monitoring, so that I can quickly identify and resolve issues with the Knowledge Base pipeline.

#### Acceptance Criteria

1. WHEN any component encounters an error, THE System SHALL log detailed error information to CloudWatch
2. THE System SHALL create CloudWatch alarms for critical failures (ingestion failures, processing errors)
3. WHEN a critical failure occurs, THE System SHALL send notifications via the existing SNS alert topic
4. THE System SHALL track metrics for ingestion job success rate, processing time, and document count
5. THE System SHALL provide CloudWatch dashboards for monitoring Knowledge Base health

### Requirement 10: Integration with Existing Infrastructure

**User Story:** As a developer, I want the Knowledge Base feature to integrate seamlessly with existing podcast processing infrastructure, so that it requires minimal changes to current workflows.

#### Acceptance Criteria

1. THE System SHALL deploy as a new CDK stack following existing project patterns in scripts/cdk-processing/
2. THE System SHALL reuse existing IAM roles where appropriate or create new roles following least-privilege principles
3. THE System SHALL integrate with the existing EventBridge rule for transcription completion (transcriptionCompletionRule)
4. THE System SHALL use the existing SNS topics for notifications (notificationTopic and alertTopic)
5. THE System SHALL use the existing S3 bucket (aws-french-podcast-media) for source data
6. THE System SHALL follow existing naming conventions and tagging strategies
